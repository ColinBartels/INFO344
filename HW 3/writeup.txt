Starting with the webrole, I implemented 10 webmethods. These are: startCrawling, stopCrawling, addRobots, getPageTitle, getSystemInfo, getLast10, getSize, getErrors, and getStatus.
These all allow the dashboard to get data from the worker role and send commands to it. When the user hits the update data button, the page sends jquery ajax requests to get all of the data. I used the jquery .when and .done methods that wait for all of the requests to complete before updating the page. Each webmethod grabs the necessary data from the stats table.
The startCrawling and stopCrawling methods add a message to the admin CloudQueue which the worker role checks before grabbing any messages from the other queues.
When the worker role is started for the first time, the robots.txt for each site is added to the sitemap queue. While the sitemap queue is not empty is always grabs urls from there instead of the html queue. After grabbing a url from either the sitemap queue or the html queue, it loads the page and sends it to the crawler class which does the actual parsing of the page. If the page is not found, or another web exception gets thrown, this error is logged in the table. The crawler class returns a list of urls which the worker role validates and adds to the appropriate queue. Finally, the current url that was parsed is sent to the addToTable method. This method hashes the url and inserts the hash, original url, title, and date into the table. It also updates the last 10 urls crawled and the table size.
If the crawler is not running or the queues are empty, it is set to idle and checks the admin queue every second.
